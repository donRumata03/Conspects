# Представление чисел

## 1. Целые числа

#### Заострений не будет: *БЕЗ*знаковые целые числа

Каждому битику сопоставляем вес его разряда.

Байт - ***минимальная адресуемая ячейка памяти***, минимальная, имеющая свой адрес. На ***данный*** момент техника стала менее разнообразная, более стандартизированная, поэтому везде 8 бит, но раньше бывало и 6.

Происходит завязывание программного обеспечения и железа. Сейчас никому не будет нужна железка, где байт -- не 8 бит.

Какой порядок битиков в байтике? Никакой. Это никак нельзя узнать, так как они не адресуемые.

#### Интереснее: кодирование чисел со знаком

1. Отдать один из битиков: отдать под знак. Обычно отдают самый старший. Теперь это признак знака (+-). Байт: [-127, 127]
    Так положительные и даже один из нулей будут совпадать с беззнаковым представлением.
2. Сначала кодируем вроде бы простое беззнаковое число, потом вычитаем $\approx$ половину диапазона. Но тогда ноль ... не нольный получится
3. "Двоичный дополнительный код" = "Дополнение до двух" -- старший бит имеет вес $-2^n$. Это доминирующее представление чисел со знаком. Unless overwise specified, используется именно это представление. Эта форма хороша, так как нет двух нулей, да ещё и правила сложения работают, как и ожидается. Чтобы инвертировать, нужно инвертировать все битики 
4. Дополнение до одного: Высший бит тоже имеет отрцательный вес

>  Модулярная арифметикa -- число интерпретируется как значение из некоторого диапазона

В пункте 3 происходит то же самое, только со сдвинутым диапазоном.

Если присвоить значение переменной ```int``` $\longrightarrow$ ```unsigned int```, то при положительном битики не изменятся, а чтобы получить, например, самое большое положительное значение - это перевод -1 в unsigned-тип.

Удобно, когда можно временно выходить за границы диапазона, если знаешь что делать: например, 250 + 10 - 20 -- даже в unsigned byte типе получится то, что планировали: 240.

По стандарту **C++20**, int должен быть в дополнении до 2х (как минимум 16 бит. На линуксе - ровно так) (сделали исследование, только одна компания делает не так, причём только на эмуляции), но стандарт С писался тогда, когда были разные представления, поэтому стандарт очень много чего допускал (а именно, что главное - чтобы можно было как-либо хранить хотя бы числа, характерные для 16 бит).

> Интересный вариант: кодировать числа в порядке $0 \longrightarrow -1 \longrightarrow 1 \longrightarrow -2 \longrightarrow \ldots$. 
>
> Это хоршо тем, что можно непосредственно записать битики малоразрядного числа в многоразрядное, и получится то же значение.
>
> Казалось бы, хранить это сложно. Ноо. Какие последовательности бит соответствуют отрицательным числам?  $1, 3, 5, \ldots \Longleftrightarrow \mathbf{000}1, \mathbf{001}1, \mathbf{010}1, \mathbf{011}1, \ldots$   
>
> Нетрудно заметить, что теперь младший бит отвечает за знак, а остальные - за модуль.

Рассмотрим систему, в которой основание равно $-2$. 

Биты: $1, -2,~ 4, -8,~ 16, -32 \ldots$

$value \in [\dots] $

Если чётное число, то отрицательных значений вдвое больше, чем положительных. Иначе -- наоборот.

___

На заре компьютеростроения система счисления была 10.

Первые компьютеры и арифмометры использовали десятичную систему. Ее минусы:

- Сложнее организовать арифметические операции, чем в двоичной

- Запись одних и тех же чисел требует большего количество информации
- Следовательно нужно больше железа

Математики посчитали, что самое эффективное основание - e (число Эйлера). Ближайшее к нему - троичная. Но двоичную проще использовать. И много алгоритмов и готовых программ.

С некоторой вероятностью, когда-нибудь человечество перейдёт на неё, но пока что мы в потенциальной яме.



## Дробные числа без плавающей точки

Как переводить дробную часть? Просто идти в порядке уменьшения дроби $\left( \frac{1}{2}, \frac{1}{4}, \ldots \right)$

Как перевести, например, 1/10?
$$
0.1_{10} = \frac{1}{16} + \frac{1}{32} + \ldots
$$
Для перевода десятичной дроби 0.1 в двоичную систему, необходимо  выполнить последовательное умножение дроби на 2, до тех пор, пока  дробная часть не станет равной 0 или пока не будет достигнута заданная  точность вычисления. Получаем:
$$
0.1 ∙ 2 = 0.2 (0) \\ 
0.2 \cdot 2 = 0.4 (0) \\
0.4 \times 2 = 0.8 (0) \\
0.8 ∙ 2 = 1.6 (1) \\
0.6 ∙ 2 = 1.2 (1) \\
0.2 ∙ 2 = 0.4 (0) \\
0.4 ∙ 2 = 0.8 (0) \\
0.8 ∙ 2 = 1.6 (1) \\
0.6 ∙ 2 = 1.2 (1) \\
0.2 ∙ 2 = 0.4 (0) \\
0.4 ∙ 2 = 0.8 (0) \\
$$
Ответом станет прямая последовательность целых частей произведения. Т.е. 

$0.1_{10} = 0.00011001100_2 = 0.0(0011)_2$

Однако есть несколько способов записать это битиками.

1. Фиксированная точка: заранее сказать, сколько бит будет под целую часть, а сколько -- под дробную. Для 16 бит:
    $$
    \mathbb{ value \in \left[0, 255\frac{255}{256} \right] \cap \left\{ \frac{n}{256} \bigg| \quad n \in  N \, \right\} }
    $$
    В случае дополнения до двух:
    $$
    \mathbb{ value \in \left[-128, 127\frac{255}{256} \right] \cap \left\{ \frac{n}{256} \bigg| \quad n \in  N \, \right\} }
    $$


    Хорошо - что можно складывать с целыми числами через reinterpret_cast.
    
    При компиляции при вводе литералов нужно округлять к ближайшему числу из доступных - самый точный способ.


   Есть несколько примеров чисел. Как произвести умножение? Алгоритмом Карацубы! (Нет…)
$$
P(x) \times Q(x) = (P_1 \times x^{\frac{n}{2}} + P_2)(Q_1 \times x^{\frac{n}{2}} + Q_2) = P_1 Q_1 x^n + (P_1 Q2 + P_2 Q_1)x^{\frac{n}{2}} + P_2 Q_2
$$
   10 - 0xA0000

   1   - 0x10000

   0.5 - 0x08000

$$
b = \ldots \\
   c = a \times b = whole \\
   
   \begin{gather}
   	a = whole_a ~+~ \frac{fraction_a}{2^n} \\
   	code_a =  (whole_a << 2^n) \parallel (fraction_a)
   \end{gather}\\
$$
   Mathematically, 
$$
   true_a = a = \frac{code_a}{2^n} \\
   true_b = b = \frac{code_b}{2^n} \\
   a \times b = \frac{code_a \times code_b}{2^{2n}}
$$

> Можно поднять промежуточную точность, производя операции в более крупном типе. Делим на $2^{2n}$ через сдвиг.

___

   Домашние задания:

- Задание простое: сделать деление.

- Задание сложнее: напечатать число в 10-ной СС с округлением до 2-х знаков после запятой



##### 	Задание 1

```c++
struct frac_number {
    uint32_t whole_part;
    uint32_t fractional_part;
    
    frac_number (uint64_t lng) 
        : whole_part(uint32_t(lng >> (8 * sizeof(fractional_part)) )),
          fractional_part(uint32_t(lng)) 
    {    
    }
    
    uint64_t to_long() {
        return uint64_t(whole_part) << (8 * sizeof(fractional_part)) 
            || uint64t(fractional_part);
    }    
};

frac_number divide(frac_number fn1, frac_number fn2) {
    return frac_number(to_long(fn1), to_long(fn2));
}

// ____________________________________________________________

std::string to_decimal(uint64_t number) {
    std::string res;
    while (number > 0) {
        res += number % 10;
        number /= 10;
    }
    std::reverse(res.begin(), res.end());
    return res;
}

std::string output_frac_decimal(frac_number num, int precision) {
    std::string res = to_decimal(num.whole_part) + ".";
    
    auto frac_times_2n = uint64_t(num.fractional_part);
    auto temp = frac_times_2n;
    for (int i = 1; i <= precision + 1; i++) {
        temp *= 10;
        auto dig = temp >> n;
        res += '0' + dig;
        temp -= dig << n;
    }
    if (res.back() >= '0' + 5) {
        res.pop_back();
        res.back() += 1;
    }
    else {
        res.pop_back();
    }
    return res;
}

```



###### Обещанное решение за одну строчку (поддержка чисел):

```c++
printf("%s%u.%02u", a < 0 ? "-" : "", abs(a)/0x10000u, abs(a)%0x1000u*100 / 0x1000);
```

Если один целочисленный тип больше себя, то число без знака



#### Алгоритмы "в столбик"

##### Посчитаем корень квадратный в столбик.

Начнём с десятичной.

root(2, 23409)

- Разбиваем на пары начиная с младшего разряда: 2'34'09, количество цифр в ответе = 3

- Угадываем первую (старшую) цифру, это 1.

- Вычитаем из макс разряда это, дописываем две цифры.

- Далее ищем такое x, которое по правилам умножения "2 * (то, что было)"_x * x, то есть  будет давать максимальное, больше чем 
  $$
  y^2 = (10y_1 + y_2)^2 = 100y_1^2 + 20y_1y_2+y_2^2 = 100 y_1^2 + (20y_1 + y_2) \cdot y_2
  $$

Под $y_1$ понимается либо первая цифра, либо промежуточный результат, под $y_2$ - всё остальное. 100 - значит пропустить две цифры 

На каждом шаге мы берём максимально большое число, которое не превышает имеющегося.

> Занание на уроке: считаем в двоичной системе. Порпобуем н

Алгоритмы в столбик работают в любой позиционной системе счисления.

## Числа с плавающей точкой

> Если с числами без плавающей точки можно как-то работать через небольшие костыли + целочисленные операции, то с плавающей точкой так не получится 

Вспомним запись числа в "показательной" форме
$$
\overset{mantissa}{\overbrace{1.3402}} \times \overset{exponent}{\overbrace{10^{-24}}}
$$


$$
5.375_{10} \rightarrow 101.011_2 \rightarrow 1.01011_2 \times 2_{10}^{2_{10}}
$$
Для кодирования такого числа нужно закодировать примерно два целых числа примерно со знаком

Можно по-разному распределить биты между мантиссой и экспонентой, но относительная точность  всегда постоянна.

Все процессоры делали кто во что горазд, и была жуткая несовместимость.

Исполнение одной и той же программы на разном железе могло быть разным.

А ещё число может быть представимо на одной железке

> Поэтому выпустили стандарт ***IEEE - 754***. 

Стандарт обновляется, есть даже поддержка десятичной плавающей точки

Мантисса кодируется как бит под знак

Экспонента кодируется как смещение с округлением смещения вниз (то есть смещение = $-(2^{(n - 1)} - 1)$)


| Название         | bits | exp  |
| ---------------- | ---- | ---- |
| half             | 16   | 5    |
| single precision | 32   | 8    |
| double precision | 64   | 11   |
| quad precison    | 128  | 15   |

Мантисса в двоичной системе всегда оказывается в виде 1.xxxxxx...

Поэтому по стандарту 
$$
5.375_{10} \rightarrow 101.011_2 \rightarrow 1.01011_2 \times 2_{10}^{2_{10}} \rightarrow
$$
$\overset{sign~bit}{\overbrace{0}}|\overset{exponent = 2}{\overbrace{10001}}|0101100000$

Точность представления - 11 бит (бит знака не учитываем, зато знаем первый бит, его не кодируя)

> Fixed point: $\pm 32768; min\_frac = \frac{1}{65536}$
>
> Floating point, single precision: $\pm 10^{38}, 10^{-38}$
>
> Floating point, double precison: $\pm 10^{308}, 10^{-308}$

Все нули в экспоненте - это специальный случай.

Запишем самое маленькое и второе по маленькости floating point число:
$$
\begin{gather}
	a = 1.00\dots001 \times 10^{-14} \\
	b = 1.00\dots000 \times 10^{-14} \\
	(a == b) == false \\
	((a - b) == 0) = true(!!)
\end{gather}
$$

Поэтому, когда все биты экспоненты - нули, кодируется "***денормализованное число***". Оно соответствует значению самой маленькой обычной экспоненты.

Ведущий ноль в этом случае подразумевается, его мы не кодируем.

Денормализованые числа ещё и расширяют диапазон на количество битов в мантиссе.

Но у самого маленького денормализованного числа, например, относительная точность 

Компьютер при операции пытается привести к номализованному виду, если не полуачется, то к **де**нормализованному, если и это не получается - ответ = 0.

___

Если экспонента равна 1111111....1, то:

- $mantissa = 0 \Longrightarrow \pm \infty$, те операции, которые ожидаются, будут давать бесконечность, потом она будет себя вести как бесконечность, а те, которые будут просто давать огромные числа, тоже в переходят в $\infty$.
- $mantissa != 0 \Longrightarrow NaN$. $NaN$ - это такие хитрые зверьки, которые живут по своим правилам. Это у чисел есть знак, а у зверьков - нет. Обладают удивительными свойствами выживаемости. Даже деление его на бесконечность  или умножение на ноль его не впечатлит. Зачем они нужны? Чтобы сообщать об ошибках. Если делить ноль на ноль, брать корень из откровенно отрицательного числа и т.д., появляются зверьки. Если железо умеет бросать исключения, можно это настроить. Лайфхак: можно проинициализировать память этими числами и посмотреть, не использовалась ли инициализированная память (не NaN ли в ответе?)



Зверьки бывают разными: залёные и красные:

Бывают тихие и сигнальные зверьки: $qNaN, sNaN$. Обычно когда старший бит мантиссы - ноль, это сигнальный, иначе - тихий. 

Железо не порождает сигнальным зверьков. Если можно кинуть исключение, оно это сделает.  Это делается только руками.

Есть функция nan(char), она обычно записывает этот char в мантиссу.

### Заметки по тесту №2

#### Округление

Округление - всегда до какого-то разряда.

Бывает округление к нулю:
$$
3.456 \to 3.45 \\
3.456 \to 3.45
$$
Ещё можно округлять от нуля, к бесконечности и к минус бесконечности.



Далее - вверх-вниз -- в смысле по модулю.

Бывает, конечно, округление к ближайшему (типа $\{ 1, 2, 3, 4 \} \to 0; \{ 5, 6, 7, 8, 9 \} \to 10$). Но тогда даже для равномерно распределённых чисел среднее из округлённых чисел будет больше, чем среднее для исходных, потому что для нуля в $\frac{4}{5}$ раза меньше вариантов, которые в него переходят. 

Способ, которое это фиксит, называется ***округление к ближайшему чётному*** - если больше половины округляемого разряда, округляем вверх, если *строго* меньше, то вниз, а если равно, то так, чтобы созраняемый разряд стал чётен. Иначе это можно запсать так: "*к ближайшему, иначе - к чётному*". Заметим, что это правило работает без привязки к базе системы счисления.

В случае с двоичной системой считсления - если округляемый разряд равен нулю, то сразу вниз, а если единица тогда смотрим, есть ли что-то ненулевое после неё. Если есть, то 
$$
0.011|01011 \searrow 0.011 \\
0.011|1 \nearrow 0.100
$$


> По умолчанию в операциях с floating-point числами происходит округление к ближайшему чётному.

#### Как надо было решать тест?

Сначала переводим в представление с фиксированной точкой (округляем двоичные дроби, если они не помещаются, к ближайшему чётному). Затем - переводя в более старший  тип, проводим операции, смотрим, поместится ли в исходный тип. Если да, то сразу делим на поправку, проводя сдвиг с правильным округлением. Если нет, то 

- В случае с арифметикой с насыщением -  в ответе записываем максимальное значение этого типа с нужным знаком
- В случае с модулярной - отдельно проводим операции с модулем, отдельно - со знаком, потом - записываем то в большом типе. Затем просто берём младшие разряды от этого.



Некое замечание:

```c++
float sum(float* data, size_t len) {
    float result = 0;
    for (size_t i = 0; i < len; i++) {
        result += data[i];
    }
    return result;
}
```

Проблема этого кода может быть в том, что ошибка накапливается $\Rightarrow$ лучше переводить в `double`.

> ***Warning***: не следует писать "size_t i = n - 1; i >= 0; i--", так как оно и так всегда больше нуля. Нудно написать "size_t i = n - 1; i != -1; i--".

> Как работает неявное приведение типов в С++? 
>
> - Если целый и floating-point, выбирается floating-point
> - Если два целых и разный размер, выбирается бОльший тип
> - Если одинакового размера, выбирается беззнаковый

[Суммирование Кэхэна](https://ru.wikipedia.org/wiki/%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%9A%D1%8D%D1%85%D1%8D%D0%BD%D0%B0) (пример алгоритма, который суммирует лучше):

```c++
float ksum(float* data, size_t len) {
    float s = 0, c = 0;
    for (size_t i = 0; i < len; i ++) {
		float y = data[i] - c;
        float t = s + y;
        c = (t - s) - y;
        s = t;
    }
}
```

> Лучшая точность достигается, если значения идут в отсортированном по модулю порядке.

> Есть библиотека double_double

> Если плавающая точка, то у компилятора завязаны руки, чтобы что-то оптимизировать.
>
> Например, даже результат сложения зависит от порядка сложения слагаемых.

В компиляторах есть ключики: `strict` (строго по стандарту) или `fast` (максимально быстро, можно подзабить на стандарт).

Если скомпилировать `ksum` с ключом `fast`, то получится изначальная функция `sum`.

Промежуточные вычисления (по типу `(t - s) - y`) могут проводиться в разных типах, не всегда понятно, в каком именно. Говорят для этого есть какой-то дефайн, но на 64 и 32 битном компьютере может быть по-разному
