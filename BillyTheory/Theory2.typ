#import "billy_theory.typ": *

 
#show: project.with(
  title: 
  "Теоретический конспект по теорверу"
)

// \section{Числовые характеристики случайных величин}

// \begin{definition}
//     Пусть $X$ — случайная величина. Тогда её математическим ожиданием называется число
//     \begin{equation}
//         \E X = \int_{\sR} x \d P_X(x)
//     \end{equation}
// \end{definition}

// Rewrite in typst ↑↑

= Числовые характеристики случайных величин

#definition[
    Пусть $X$ — случайная величина. Тогда её математическим ожиданием называется число
    $
        EE X = integral_RR x dif F_X(x)
    $
    (интеграл Лебега-Стилтьеса)
]

#remark[
    Если $X$ — дискретная случайная величина, то
    $
        EE X = sum_(x in RR) x P_X(x)
    $
]

#remark[
    Если $X$ — абсолютно непрерывная случайная величина, то
    $
        EE X = integral_RR x p_X(x) dif x
    $
]

#property[
    Если $X$ — случайная величина, то $EE X$ — число.
]
 
#definition[
    Пусть $X$ — случайная величина. Тогда её дисперсией называется число
    $
        Var X = DD X = EE (X - EE X)^2
    $

    Стандартным отклонением случайной величины $X$ называется число $sigma_X = sqrt(Var X)$.
    Она часто используется вместо дисперсии, потому что она имеет ту же размерность, что и $X$.
]

#definition[
    Пусть $X$ — случайная величина. Тогда для $alpha in (0, 1)$
    $
        q_alpha — "квантиль порядка" alpha — "число, такое что" cases(
            P(x >= q_alpha) >= 1 - alpha,
            P(x <= q_alpha) >= alpha
        )
    $
]

Для непрерывной случайной величины $X$ квантиль порядка $alpha$ — это решение уравнения $F_X(x) = alpha$. Если $F_X$ строго возрастает, то $q_alpha = F_X^(-1)(alpha)$.

Для дискретной случайной величины $X$ квантиль порядка $alpha$ — это минимальное $x$, такое что $P_X(x) >= alpha$.

#definition[
    Медиана случайной величины $med X$ — это квантиль порядка $1/2$.
]

#theorem[
    $
    med X = limits(argmin)_(x in RR) EE |X - x|
    $
]

Матожидание тоже кое-что оптимизирует, но не так круто.

#theorem[
    $
    EE X = limits(argmin)_(x in RR) EE (X - x)^2
    $
]

Почему не так круто, спросите вы? Потому что матожидание — это не медиана, а среднее. А среднее — это для средних, посредственных людей. А медиана — это для лучших. © Copilot


#definition[
    Момент порядка $k$ случайной величины $X$ — это число $EE X^k$.
]

#definition[
    Центральный момент порядка $k$ случайной величины $X$ — это число $EE (X - EE X)^k$.
]

#definition[
    Абсолютный момент порядка $k$ случайной величины $X$ — это число $EE |X|^k$.
]

#definition[
    Абсолютный центральный момент порядка $k$ случайной величины $X$ — это число $EE |X - EE X|^k$.
]

#example[
    Коэфициент асимметрии случайной величины $X$ — это, с точностью до коэфициента, центральный момент порядка $3$: $EE (X - EE X)^3 / sigma^3$.

    Коэфициент эксцесса случайной величины $X$ — это, с точностью до коэфициента, центральный момент порядка $4$: $EE (X - EE X)^4 / sigma^4 - 3$. Минус три потому что мы хотим, чтобы эксцесс нормального распределения был нулевой.
]

#definition[
    Мода случайной величины $X$ — это число $argmax_(x in RR) p_X(x)$.
]



